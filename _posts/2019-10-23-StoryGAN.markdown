---
layout:     post
title:      "《StoryGAN: A Sequential Conditional GAN for Story Visualization》读书笔记"
subtitle:   "Wants to make reading books interesting?"
date:       2019-10-23
author:     "Haoyue"
header-img: "img/post-bg-js-version.jpg"
tags:
    - 读书
    - 生活
---
## 《StoryGAN: A Sequential Conditional GAN for Story Visualization》读书笔记

这是一篇发表在CPVR2019的论文。论文中提出了**故事可视化**的新任务，即基于给定句子生成一系列对应的图像，每张图像对应一个句子。文中作者提出了一个故事-图像序列生成模型——StoryGAN，它基于条件序列生成对抗网络。实验结果表明，由StoryGAN生成的图像序列在图像质量、语义连贯性以及人为评价方面都超过了目前最好的模型。

## 一、Challenges
阅读小说是一件很有趣的事情，但是没有插图的故事往往索然无味。特别是儿童书籍，缺乏插图可能会让故事变得无聊。让模型基于自然语言学习，然后生成有意义且连贯的图像序列是一个有挑战的任务。
第一，图像序列必须连贯且完整地描述整个故事。这与文本-图像生成紧密相关，即根据简单的描述生成图像。
第二，如何有逻辑地呈现故事情节。具体来说，图像中个体的外观和背景布局必须根据故事情节推进，并以恰当的方式呈现。

## 二、StoryGAN
整体上看，StoryGAN主要由Story Encoder、Context Enocder、Image Generator、Image Discriminator和Story Discriminator组成，其中Context Encoder中又包含GRU单元和Text2Gist单元。

![img](https://github.com/sunshinemingo/sunshinemingo.github.io/raw/master/img/image_md/image_17.png)

#### 1. Story Encoder
Story Encoder将故事描述语句$S$随机映射到一个低维的向量空间，得到的向量不仅包含了$S$全部的信息，同时还作为Context Encoder隐藏单元的初始值。
为了解决原先故事空间不连续的问题使用了随机采样，不仅在故事的可视化工作中使用了一种更加紧凑的语义表示，而且还为图像的生成过程增加了随机性。同时为了增强条件流行在潜在语义空间上的平滑性，避免生成模式坍缩到单个的生成点上，引入了一个正则化项，即计算生成分布和标准正态分布之间的KL散度。
![img](https://github.com/sunshinemingo/sunshinemingo.github.io/raw/master/img/image_md/image_18.png)

#### 2. Context Encoder
Context Encoder基于深层RNN网络构建，包含两个隐藏层，GRU单元和Text2Gist单元，其中，Text2Gist单元是GRU单元的变体。
在时间步$t$，GRU层接受句子$s_t$和等距高斯噪声$\epsilon_t$作为输入，并输出向量$i_t$。Text2Gist 单元将GRU的输出$i_t$和Story Encoder的输出$h_{t-1}$结合（$h_{t-1}$来自Story Encoder），生成$O_t$。$O_t$编码了在时间步$t$需要生成图像的所有必要信息。
![img](https://github.com/sunshinemingo/sunshinemingo.github.io/raw/master/img/image_md/image_19.png)

$h_t$由Text2Gist更新，以反映潜在的语境信息变化。$O_t$是Gist向量（高维特征向量），它分别结合了来自$h_{t-1}$的全局语境和$i_t$在时间步t的局部语境信息。Story Encoder初始化了$h_0$，而$g_0$则是从等距高斯噪声分布中随机采样得到。然后$O_t$输入到Image Generator中，生成图像样本${\hat{x}}_t$。在时间步$t$，Text2Gist详细的更新公式如下所示：
![img](https://github.com/sunshinemingo/sunshinemingo.github.io/raw/master/img/image_md/image_20.png)

#### 3. Discriminators
模型中存在两个判别器：基于深度神经网络的Image Discriminator，保持生成图像的局部一致性；基于多层感知机的Story Discriminator，保持生成图像的全局一致性。
Image Discriminator只是用来判别生成图像是否足够真实；Story Discriminator的原理如下图所示：
![img](https://github.com/sunshinemingo/sunshinemingo.github.io/raw/master/img/image_md/image_21.png)
将输入的语句和相应的生成图像分别降维得到它们的特征向量，然后各自拼接起来送到一个包含Sigmoid的全连接网络中，判别生成的图像是否在语义上具有连贯性。

#### 4. Summary
StoryGAN的目标函数为：
![img](https://github.com/sunshinemingo/sunshinemingo.github.io/raw/master/img/image_md/image_22.png)
其中，
![img](https://github.com/sunshinemingo/sunshinemingo.github.io/raw/master/img/image_md/image_23.png)
算法描述如下：
![img](https://github.com/sunshinemingo/sunshinemingo.github.io/raw/master/img/image_md/image_24.png)

## 三、Experiment
实验中与StoryGAN进行比较的模型是ImageGAN、SVC和SVFN，其中，SVC和SVNF是StoryGAN的简化形式，使用的数据集为CLVER-SV Dataset和Pororo-SV Dataset。
#### 1. CLEVR-SV Dataset
CLEVR-SV数据集基于CLEVR数据集进行修改； 
（1）将一个故事中的目标数量最多限制在4个； 
（2）物体由金属或橡胶制成，有八种颜色和两种尺寸； 
（3）物体可以是圆柱体、立方体或球体； 
（4）每次增加一个物体，直到一个故事中有四幅图像序列。 
结果表明，ImageGAN不能保持“故事”的一致性，当物体的数量增加时，它会混淆物体的属性；SVC模型虽然没有出现一致性问题，但是在SVC生成的图像序列中出现了一个难以置信的外置图像；SVFN能在一定程度上缓解SVC出现的问题，但不能完全缓解。相比之下，StoryGAN生成的图像序列更加合理。
此外，文中还比较了生成的图像与ground-truth之间的结构相似指数(SSIM)，结果显示，StoryGAN显著地优于其他模型。
![img](https://github.com/sunshinemingo/sunshinemingo.github.io/raw/master/img/image_md/image_25.png)

#### 2. Pororo-SV Dataset
Pororo-SV数据集基于Pororo数据集进行修改，将每个视频片段的描述作为故事的文本输入。对于每个视频片段，随机提取一帧画面（采样率为30Hz）作为真实的图像样本用于训练。五个连续的图像组成一个完整故事。
结果表明，ImageGAN不能生成一致的图像序列；SVC和SVFN可以在一定程度上改善生成的图像序列中的角色外观的不一致性，此外还在实验中改变了故事中角色的名字。结果显示，StoryGAN在图像质量以及故事一致性上都要优于其他模型。
![img](https://github.com/sunshinemingo/sunshinemingo.github.io/raw/master/img/image_md/image_26.png)

#### 3. Human Evaluation
在人为评价方面，文中先从视觉质量、一致性以及相关性三个方面对StoryGAN和ImageGAN进行了比较，结果显示StoryGAN明显优于ImageGAN。
接下来分别对四个模型基于特定的故事生成的图像的整体质量进行比较，发现StoryGAN的平均排名最高，而ImageGAN的排名最差。


## 四、Conclusion
论文中提出的故事-图像序列生成模型——StoryGAN能够基于给定的故事，生成一系列的图像，而且与其他模型相比，StoryGAN在图像质量、语义连贯性以及人为评价方面都超过了目前最好的模型。相信随着图像生成模型的改进，故事可视化的质量也会提高。
