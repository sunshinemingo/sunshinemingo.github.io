---
layout:     keynote
title:      "StoryGAN: A Sequential Conditional GAN for Story Visualization"
subtitle:   "Keynote: JavaScript Modularization Journey"
date:       2019-10-23
author:     "Haoyue"
header-img: "img/post-bg-js-version.jpg"
tags:
    - 读书
    - JavaScript
---
## 《StoryGAN: A Sequential Conditional GAN for Story Visualization》读书笔记

阅读小说是一件很有趣的事情，但是没有插图的故事往往索然无味。特别是儿童书籍，缺乏插图可能会让故事变得无聊。

![img](https://github.com/sunshinemingo/sunshinemingo.github.io/raw/master/img/image_md/image_17.png)

StoryGAN，一个序列化的生成对抗网络。首先，Story Encoder将故事描述$S$随机映射到一个低维的向量空间，得到的向量不仅包含了$S$全部的信息，同时还作为Context Encoder隐状态的初始值。另外为了解决原先故事空间不连续的问题使用了随机采样，不仅在故事的可视化工作中使用了一种更加紧凑的语义表示，而且还为图像的生成过程增加了随机性。同时为了增强条件流行在潜在语义空间上的平滑性，避免生成模式坍缩到单个的生成点上，引入了一个正则化项，即计算生成分布和标准正态分布之间的KL散度。对于Context Encoder，包含两个隐藏层，GRU cell和Text2Gist cell，在时间步$t$，GRU层接受句子$s_t$和等距高斯噪声$\epsilon_t$作为输入，并输出向量$i_t$。Text2Gist 单元将GRU的输出$i_t$和Story Encoder的输出$h_{t-1}$结合（$h_{t-1}$来自Story Encoder），生成$O_t$。$O_t$编码了在时间步$t$需要生成图像的所有必要信息。$h_t$由Text2Gist更新，以反映潜在的语境信息变化。$O_t$是Gist向量（高维特征向量），它分别结合了来自$h_{t-1}$的全局语境和$i_t$在时间步t的局部语境信息。Story Encoder初始化了$h_0$，而$g_0$则是从等距高斯噪声分布中随机采样得到。然后$O_t$输入到Image Generator中，生成图像样本${\hat{x}}_t$，最后，顶部有两个判别器，Image Discriminator和 Story Discriminator分别判断每个图像-句子对和图像-序列-故事对是真实数据还是生成数据。

GAN通过框架中（至少）两个模块：生成模型(Generative Model)和判别模型(Discriminative Model)的互相博弈学习产生相当好的输出。判别模型需要输入变量，通过某种模型来预测。生成模型是给定某种隐含信息，来随机产生观测数据。G是一个生成图片的网络，它接收一个随机的噪声z，通过这个噪声生成图片，记做G(z)。D是一个判别网络，判别一张图片是不是“真实的”。它的输入参数是x，x代表一张图片，输出D(x)代表x为真实图片的概率，如果为1，就代表100%是真实的图片，而输出为0，就代表不可能是真实的图片。在训练过程中，生成网络G的目标就是尽量生成真实的图片去欺骗判别网络D。而D的目标就是尽量把G生成的图片和真实的图片分别开来。这样，G和D构成了一个动态的“博弈过程”。
