---
layout:     post
title:      "《Graph Neural Networks- A Review of Methods and Applications》读书笔记"
subtitle:   "Analysis of the real usage and impact of GitHub reactions"
date:       2019-10-30
author:     "Haoyue"
header-img: "img/post-bg-graph-networks.jpg"
tags:
    - 读书
    - 生活
---

## 《Graph Neural Networks- A Review of Methods and Applications》读书笔记

这是一篇发表在SBES2019，巴西第三十三届软件工程研讨会论文集中的论文。许多学习任务都需要处理包含丰富元素间关系信息的图数据。物理系统建模、分子指纹学习、蛋白质界面预测、疾病分类等都需要一个模型从图输入中学习。在文本、图像等非结构化数据学习领域，对提取出来的句子依赖树、图像场景图等结构进行推理是一个重要的研究课题，也需要图形推理模型。图神经网络(GNNs)是一种连接模型，它通过在图节点之间传递消息来捕获图的依赖性。与标准的神经网络不同，图神经网络保留了一种状态，这种状态可以用任意深度表示来自其邻域的信息。虽然原始的gnn很难进行定点训练，但是最近在网络体系结构、优化技术和并行计算方面的进展使使用它们进行成功的学习成为可能。近年来，基于图卷积网络(graph convolutional network, GCN)、图注意网络(graph attention network, GAT)、门控图神经网络(gated graph neural network, GGNN)等图神经网络变体的系统在上述许多任务上都表现出了突破性的性能。在这项调查中，我们详细回顾了现有的图神经网络模型，系统地分类应用，并提出了四个有待进一步研究的问题。

## 一、Background
图是一种数据结构，它对一组对象(节点)及其关系(边)进行建模。作为机器学习中唯一一个非欧几里得数据结构，图的分析主要是节点分类、关系预测和聚类。由于其较好的性能和可解释性，GNN已经成为一种广泛应用的图分析方法。

GNN的第一个动机源于卷积神经网络(CNN)。CNN模型的关键点在于局部连接、权值共享和多层使用。这些关键点对于解决图领域中的问题也很重要。然而CNN只能在规则的欧几里得数据上运行，如图像(2维网格)和文本(1维序列)。由于很难定义局部卷积滤波器以及池化操作，因此阻碍了CNN模型从欧几里得空间到非欧几里得空间的转换。

GNN的另一个动机来自图嵌入(Graph Embedding)，它学习图像中节点、边或子图的低维向量空间表示。在图分析中，传统的机器学习方法依赖于人为设计特征，并且由于其不灵活性和高成本而受到限制。DeepWalk、node2vec、LINE、TADW等方法在网络表示学习领域取得了很大的成功。然而，这些方法计算效率低下且泛化能力差。

基于CNN和Graph Embedding，提出了GNN。它可以对由元素及其相关性组成的输入和/或输出进行建模。此外，图神经网络可以使用RNN内核同时对图的扩散过程进行建模。

类似于CNN和RNN的标准神经网络不能正确地处理图形输入，因为它们以特定顺序堆叠节点的特征。为了解决这个问题，GNN分别在每个节点上传播，而忽略节点的输入顺序。图形中的边表示两个节点之间的依赖关系，然而在标准神经网络中，这种依赖关系被认为是节点的特征。通常，GNN通过节点附近状态的加权总和来更新节点的隐藏状态。此外，推理是高级人工智能的一个非常重要的研究主题，人脑的推理过程几乎基于从日常经验中提取的图形。标准神经网络通过学习数据分布能够生成合成图像和文档，但他们仍然无法从大型实验数据中学习推理图。

文章对图神经网络进行了广泛的总结，并做出了以下贡献：
1. 文章详细介绍了图神经网络的经典模型，主要包括其原始模型，不同的变体和几个通用框架。文中对该领域中的各种模型进行了研究，并提供了统一的表示形式，以呈现不同模型中的不同传播步骤。通过识别相应的聚合器和更新器，能够轻松地区分不同的模型。
2. 文章将图神经网络的应用系统地归类为结构化场景、非结构化场景和其他场景，并介绍了不同场景中的主要应用。
3. 本文为未来的研究提出四个未解决的问题。文章对每个问题进行了详细分析，并提出了未来的研究方向。

## 二、Models
### 1. Graph Neural Networks
在图中，每个节点是由其特征和相关节点定义的。GNN的目标是学习一个状态嵌入，其中包含每个节点的邻域信息。GNN中有两种函数，局部转移函数(local transition function)和局部输出函数(local output function)。局部转移函数在所有节点中共享，根据输入邻域更新节点状态，局部输出函数描述输出是如何生成的。

但是，GNN模型仍然存在一定的限制：
1. 针对固定点迭代更新节点的隐藏状态效率低下。如果放宽对不动点的假设，可以设计一个多层GNN来得到节点及其邻域的稳定表示。
2. GNN在迭代中使用相同的参数，而很多流行的神经网络在不同的层中使用不同的参数，分层进行特征的提取。此外，节点隐藏状态的更新是一个顺序过程，可以从RNN内核(如GRU和LSTM)中获益。
3. 图形的边缘包含一些特征信息，这些特征无法在原始GNN模型中建模。此外，如何学习边缘的隐藏状态也是一个重要的问题。
4. 如果关注节点的表示而不是图形的表示，则不适合使用固定点，因为固定点上的表示分布在数值上较为平滑，并且对于区分每个节点的信息量较小。

### 2. Variants of Graph Neural Networks
图神经网络的不同变体的概览如下图所示：
![img](https://github.com/sunshinemingo/sunshinemingo.github.io/raw/master/img/image_md/image_36.png)

#### (1)Graph Types
##### Directed Graphs
无向边可以看作是两条有向边，表示两个节点之间存在关联。但是，有向边能够比无向边表示更多的信息。DGP使用两种权重矩阵，合并更加精确的结构信息。

##### Heterogeneous Graphs
异构图中有不同种类的节点，最简单的处理方法是将每个节点的类型转换为与原始特征连接的one-hot特征向量。GraphInception在传播中引入了元路径(metapath)的概念，使用元路径，可以根据邻居的节点类型和距离进行分组。对于每个邻居组，GraphInception将其看作同构图中的子图进行传播，并将不同同构图的传播结果连接起来以进行集合节点的表示。HAN利用了节点级别和语义级别的注意力，并且能够同时考虑节点重要性和元路径。

##### Graphs with Edge Information
图中的每条边具有额外的信息，例如边的权重或者类型。有两种方法处理这种图形；
1. 将原始图像转换成二分图。原始边变成节点，并且将一条原始边拆分成两条新边，这意味着在边缘节点和开始/结束节点之间有两条新边。此处提到了G2S模型。
2. 针对不同类型的边上的传播使用不同的权重矩阵。r-GCN引入两种正则化以减少用于建模关系量的参数数量：basis分解和block-diagonal分解。

##### Dynamic Graphs
动态图具有静态的图形结构和动态的输入。为了获取两种信息，DCRNN和STGCN首先通过GNN收集空间信息，然后将输出输入到序列模型中。不同地，Structural-RNN和ST-GCN同时收集空间信息和时间信息，它们通过时间连接扩展了静态图结构，因此可以在扩展图上应用传统的GNN。

#### (2)Propagation Types
GNN模型的不同变体的比较如下图所示。这些变体使用不同的聚合器收集每个节点的邻居信息，使用特定的更新程序更新节点的隐藏状态。
![img](https://github.com/sunshinemingo/sunshinemingo.github.io/raw/master/img/image_md/image_37.png)

##### Convolution
在图形领域中使用卷积操作，分为频谱法和非频谱(空间)法。
(1)频谱法使用图形的频谱表示。
**Spectral Network**通过计算图形拉普拉斯变换的特征分解，在傅里叶域中定义了卷积运算，一个信号量(每个节点的标量)和一个过滤器的乘积。这种操作会导致潜在的密集计算和非空间局部化的过滤器。M. Henaff等人试图通过引入平滑系数的参数化使光谱滤波器空间局域化。
**ChebNet**使用K-局部卷积来定义卷积神经网络，这样可以不用计算拉普拉斯的特征向量。
**GCN**将分层卷积运算限制为K = 1，以减轻节点度分布非常宽的图在局部邻域结构上的过拟合问题。
**AGCN**可以学习不同节点之间的隐含关系。AGCN学习一个“残差”图拉普拉斯算子，并将其添加到原始拉普拉斯算子矩阵中。
**GGP**解决半监督学习问题。该模型与频谱滤波方法有相似之处。
然而，在上述所有频谱法中，学习的滤波器取决于拉普拉斯特征基，而拉普拉斯特征基取决于图结构，即在特定结构上训练的模型不能直接应用于不同结构的图中。
(2)非频谱方法直接在图上定义卷积，并在空间上相邻的邻居上进行运算。非频谱方法的主要挑战是定义具有不同大小邻域的卷积运算并保持CNN的局部不变性。
**NeuralFPs**为不同度数的节点使用不同的权重矩阵。但是它不能应用于节点度较大的大规模图形。
**DCNN**扩散卷积神经网络。转换矩阵用于定义DCNN中节点的邻域。DCNN可以应用于节点分类任务和边分类任务，当应用于边分类任务时，需要将边转换为节点并增加邻接矩阵。
**DGCN**既考虑图的局部一致性，也考虑图的全局一致性。使用两个卷积网络获取局部/全局一致性，并用无监督损失整合它们。
**PATCHY-SAN**为每个节点提取并归一化为一个恰好有k个节点的邻域，然后归一化邻域作为卷积运算的感受野。
**LGCN**利用CNN作为聚合器。它在节点的邻接矩阵上进行最大池化的操作，得到前k个特征元素，然后使用一维CNN计算隐藏表示。
**MoNet**是一种非欧几里得域上的空间域模型，该模型可以概括已有的几种技术。流形上的GCNN和ACNN或者图形上的GCN和DCNN都可以表示为MoNet的具体实例。
**GraphSAGE**是一个通用的归纳框架。该框架对节点的局部邻居中的特征进行采样和聚集来生成嵌入。但是，并没有利用全部邻居集，而是通过均匀采样使用固定大小的邻居集。
**SACNN**使用单变量函数作为滤波器，可以处理欧几里得结构和非欧几里得结构数据。

##### Gate
**GGNN**在传播步骤中使用GRU，进行固定步数的递归操作，并使用反向传播计算梯度。
**Child-Sum Tree-LSTM**和**N-ary Tree-LSTM**每个Tree-LSTM单元包含输入门、输出门、记忆单元和隐藏状态，并且为每个子类设置一个遗忘门，使得每个Tree-LSTM单元可以选择性地合并每个子类的信息。如果树最多有K个分支且每个节点的所有子节点都被排好序，则可以使用N-ary Tree-LSTM。与Child-Sum Tree-LSTM相比，N-ary Tree-LSTM为每个子类引入单独的参数矩阵，可以学习更多关于单元子类状态的细粒度表示。
N. Peng等人基于关系提取任务提出Graph LSTM的变体，最大的不同是图中的边有标签，并且使用不同的权重矩阵表示不同的标签。
**S-LSTM**目的是改进文本编码，将文本转换成图，使用Graph LSTM学习表示。
X. Liang等人提出的Graph LSTM模型旨在解决语义对象解析任务。它使用置信度驱动方案自适应地选择起始节点并确定节点更新顺序，有特定的更新顺序。

##### Attention
**GAT**在传播步骤中使用注意力机制。它通过关注它的邻居来计算每个节点的隐藏状态，遵循自注意力(self-attention)策略。GAT定义了图注意力层(graph attentional layer)，通过叠加该注意力层可以构建任意的图注意力网络。此外，该注意力层使用了multi-head注意力机制来稳定学习过程。它使用了K个独立的注意力机制来计算隐藏状态，然后将其特征连接起来(或计算平均值)。
**GAAN**也使用了multi-head注意力机制，但是它使用自注意力机制聚集来自不同head的信息，以代替GAT的平均计算。

##### Skip connection
**Highway GCN**使用类似于高速公路网络的分层闸，每一层的输出与其输入的选通权重相加
**Column Network(CLN)**也利用了公路网，但是它使用不同的函数来计算门控权重。
**Jump Knowledge Network**可以学习自适应、结构感知的表示。模型从最后一层的每个节点的所有中间表示中进行选择，这使模型可以根据需要调整每个节点的有效邻域大小。此外，模型使用了concatenation, max-poolin和LSTM-attention三种方法汇总信息。

##### Hierarchical Pooling
**Edge-Conditioned Convolution(ECC)**设计了一个具有递归下采样操作的池化模块，下采样方法是根据拉普拉斯矩阵的最大特征向量的符号将图分解成两个分量。
DIFFPOOL通过在每一层训练一个分配矩阵，提出了一种可学习的层次聚类模型.

#### (3)Training Methods
##### Sampling
**GraphSAGE**使用可学习的聚合函数替换了完整的图形拉普拉斯算子，这是进行消息传递和推广到看不见的节点的关键。模型首先聚合邻域嵌入，然后与目标节点的嵌入合并，传播到下一层。通过学习到的聚合和传播功能，GraphSAGE可以为看不见的节点生成嵌入。此外，GraphSAGE使用邻域采样来缓解感受野的扩展。
PinSage提出了基于重要性的采样方法。通过模拟从目标节点开始的随机游走，选择标准化访问次数最高的前T个节点。
**FastGCN**直接对每一层的感受野进行采样，而不是采样每个节点的邻居，且根据重要性进行采样。
W. Huang等人介绍了一种参数化、可训练的采样器，可以在前一层的基础上进行分层采样。此外，该自适应采样器可以同时找到最优采样重要性和减少方差。
SSE提出了用于GNN训练的随机不动点梯度下降法。此方法将嵌入更新视为值函数，将参数更新视为值函数。在训练期间，该算法将对节点进行采样以更新嵌入，并对标记的节点进行采样以交替更新参数。

##### Receptive Field Control
J. Chen等人利用节点的历史激活作为控制变量，提出了一种基于控制变量的GCN随机逼近算法。

##### Data Augmentation
Co-Training GCN和Self-Training GCN用于扩大训练数据集。前者查找训练数据的最近邻，后者遵循类似Boosting的方法。

##### Unsupervised Training
图形自动编码器的目的是通过无监督的训练方式将节点表示为低维向量。**GAE(图形自动编码器)**首先使用GCN对图中的节点进行编码。然后使用一个简单的解码器来重建邻接矩阵，并根据原始邻接矩阵与重构矩阵之间的相似度来计算损失。
**Adversarially Regularized Graph Auto-encoder(ARGA)**运用生成对抗网络(GAN)来规范化基于GCN的图形自动编码器以遵循先前的分布。


### 3. General Frameworks

## 三、Applications



## 四、Open problems





## 五、Conclusion

