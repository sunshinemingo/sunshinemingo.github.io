---
layout:     post
title:      "《Graph Neural Networks- A Review of Methods and Applications》读书笔记"
subtitle:   "A comprehensive review of graph neural networks"
date:       2019-11-01
author:     "Haoyue"
header-img: "img/post-bg-graph-networks.jpg"
tags:
    - 读书
    - 生活
    - 学习
---

## 《Graph Neural Networks- A Review of Methods and Applications》读书笔记

图神经网络(GNNs)是一种连接模型，它在图节点之间传递消息，捕获图的依赖性。文中对现有的图神经网络模型进行了详细的回顾，系统地分类应用场景，并提出了四个有待进一步研究的问题。

## 一、Background
图是一种数据结构，它对一组对象(节点)及其关系(边)进行建模。作为机器学习中唯一一个非欧几里得数据结构，图的分析主要是节点分类、关系预测和聚类。由于其较好的性能和可解释性，GNN已经成为一种广泛应用的图分析方法。

GNN的第一个动机源于卷积神经网络(CNN)。CNN模型的关键点在于局部连接、权值共享和多层使用。这些关键点对于解决图领域中的问题也很重要。然而CNN只能在规则的欧几里得数据上运行，如图像(2维网格)和文本(1维序列)。由于很难定义局部卷积滤波器以及池化操作，因此阻碍了CNN模型从欧几里得空间到非欧几里得空间的转换。

GNN的另一个动机来自图嵌入(Graph Embedding)，它学习图像中节点、边或子图的低维向量空间表示。在图分析中，传统的机器学习方法依赖于人为设计特征，并且由于其不灵活性和高成本而受到限制。DeepWalk、node2vec、LINE、TADW等方法在网络表示学习领域取得了很大的成功。然而，这些方法计算效率低下且泛化能力差。

基于CNN和Graph Embedding，提出了GNN。它可以对由元素及其相关性组成的输入和/或输出进行建模。此外，图神经网络可以使用RNN内核同时对图的扩散过程进行建模。

类似于CNN和RNN的标准神经网络不能正确地处理图形输入，因为它们以特定顺序堆叠节点的特征。为了解决这个问题，GNN分别在每个节点上传播，而忽略节点的输入顺序。图形中的边表示两个节点之间的依赖关系，然而在标准神经网络中，这种依赖关系被认为是节点的特征。通常，GNN通过节点附近状态的加权总和来更新节点的隐藏状态。此外，推理是高级人工智能的一个非常重要的研究主题，人脑的推理过程几乎基于从日常经验中提取的图形。标准神经网络通过学习数据分布能够生成合成图像和文档，但他们仍然无法从大型实验数据中学习推理图。

文章对图神经网络进行了广泛的总结，并做出了以下贡献：

1.文章详细介绍了图神经网络的经典模型，主要包括其原始模型，不同的变体和几个通用框架。文中对该领域中的各种模型进行了研究，并提供了统一的表示形式，以呈现不同模型中的不同传播步骤。通过识别相应的聚合器和更新器，能够轻松地区分不同的模型。

2.文章将图神经网络的应用系统地归类为结构化场景、非结构化场景和其他场景，并介绍了不同场景中的主要应用。

3.本文为未来的研究提出四个未解决的问题。文章对每个问题进行了详细分析，并提出了未来的研究方向。

## 二、Models
### 1. Graph Neural Networks
在图中，每个节点是由其特征和相关节点定义的。GNN的目标是学习一个状态嵌入，其中包含每个节点的邻域信息。GNN中有两种函数，局部转移函数(local transition function)和局部输出函数(local output function)。局部转移函数在所有节点中共享，根据输入邻域更新节点状态，局部输出函数描述输出是如何生成的。

但是，GNN模型仍然存在一定的限制：

1.针对固定点迭代更新节点的隐藏状态效率低下。如果放宽对不动点的假设，可以设计一个多层GNN来得到节点及其邻域的稳定表示。

2.GNN在迭代中使用相同的参数，而很多流行的神经网络在不同的层中使用不同的参数，分层进行特征的提取。此外，节点隐藏状态的更新是一个顺序过程，可以从RNN内核(如GRU和LSTM)中获益。

3.图形的边缘包含一些特征信息，这些特征无法在原始GNN模型中建模。此外，如何学习边缘的隐藏状态也是一个重要的问题。

4.如果关注节点的表示而不是图形的表示，则不适合使用固定点，因为固定点上的表示分布在数值上较为平滑，并且对于区分每个节点的信息量较小。

### 2. Variants of Graph Neural Networks
图神经网络的不同变体的概览如下图所示：
![img](https://github.com/sunshinemingo/sunshinemingo.github.io/raw/master/img/image_md/image_36.png)
#### (1)Graph Types
##### a. Directed Graphs
无向边可以看作是两条有向边，表示两个节点之间存在关联。但是，有向边能够比无向边表示更多的信息。DGP使用两种权重矩阵，合并更加精确的结构信息。
##### b. Heterogeneous Graphs
异构图中有不同种类的节点，最简单的处理方法是将每个节点的类型转换为与原始特征连接的one-hot特征向量。GraphInception在传播中引入了元路径(metapath)的概念，使用元路径，可以根据邻居的节点类型和距离进行分组。对于每个邻居组，GraphInception将其看作同构图中的子图进行传播，并将不同同构图的传播结果连接起来以进行集合节点的表示。HAN利用了节点级别和语义级别的注意力，并且能够同时考虑节点重要性和元路径。
##### c. Graphs with Edge Information
图中的每条边具有额外的信息，例如边的权重或者类型。有两种方法处理这种图形；

1.将原始图像转换成二分图。原始边变成节点，并且将一条原始边拆分成两条新边，这意味着在边缘节点和开始/结束节点之间有两条新边。此处提到了G2S模型。

2.针对不同类型的边上的传播使用不同的权重矩阵。r-GCN引入两种正则化以减少用于建模关系量的参数数量：basis分解和block-diagonal分解。
##### d. Dynamic Graphs
动态图具有静态的图形结构和动态的输入。为了获取两种信息，DCRNN和STGCN首先通过GNN收集空间信息，然后将输出输入到序列模型中。不同地，Structural-RNN和ST-GCN同时收集空间信息和时间信息，它们通过时间连接扩展了静态图结构，因此可以在扩展图上应用传统的GNN。

#### (2)Propagation Types
GNN模型的不同变体的比较如下图所示。这些变体使用不同的聚合器收集每个节点的邻居信息，使用特定的更新程序更新节点的隐藏状态。
![img](https://github.com/sunshinemingo/sunshinemingo.github.io/raw/master/img/image_md/image_37.png)
##### a. Convolution
在图形领域中使用卷积操作，分为频谱法和非频谱(空间)法。

(1)频谱法使用图形的频谱表示

**Spectral Network**通过计算图形拉普拉斯变换的特征分解，在傅里叶域中定义了卷积运算，一个信号量(每个节点的标量)和一个过滤器的乘积。这种操作会导致潜在的密集计算和非空间局部化的过滤器。M. Henaff等人试图通过引入平滑系数的参数化使光谱滤波器空间局域化。

**ChebNet**使用K-局部卷积来定义卷积神经网络，这样可以不用计算拉普拉斯的特征向量。

**GCN**将分层卷积运算限制为K = 1，以减轻节点度分布非常宽的图在局部邻域结构上的过拟合问题。

**AGCN**可以学习不同节点之间的隐含关系。AGCN学习一个“残差”图拉普拉斯算子，并将其添加到原始拉普拉斯算子矩阵中。

**GGP**解决半监督学习问题。该模型与频谱滤波方法有相似之处。

然而，在上述所有频谱法中，学习的滤波器取决于拉普拉斯特征基，而拉普拉斯特征基取决于图结构，即在特定结构上训练的模型不能直接应用于不同结构的图中。

(2)非频谱方法直接在图上定义卷积，并在空间上相邻的邻居上进行运算。非频谱方法的主要挑战是定义具有不同大小邻域的卷积运算并保持CNN的局部不变性。

**NeuralFPs**为不同度数的节点使用不同的权重矩阵。但是它不能应用于节点度较大的大规模图形。

**DCNN**扩散卷积神经网络。转换矩阵用于定义DCNN中节点的邻域。DCNN可以应用于节点分类任务和边分类任务，当应用于边分类任务时，需要将边转换为节点并增加邻接矩阵。

**DGCN**既考虑图的局部一致性，也考虑图的全局一致性。使用两个卷积网络获取局部/全局一致性，并用无监督损失整合它们。

**PATCHY-SAN**为每个节点提取并归一化为一个恰好有k个节点的邻域，然后归一化邻域作为卷积运算的感受野。

**LGCN**利用CNN作为聚合器。它在节点的邻接矩阵上进行最大池化的操作，得到前k个特征元素，然后使用一维CNN计算隐藏表示。

**MoNet**是一种非欧几里得域上的空间域模型，该模型可以概括已有的几种技术。流形上的GCNN和ACNN或者图形上的GCN和DCNN都可以表示为MoNet的具体实例。

**GraphSAGE**是一个通用的归纳框架。该框架对节点的局部邻居中的特征进行采样和聚集来生成嵌入。但是，并没有利用全部邻居集，而是通过均匀采样使用固定大小的邻居集。

**SACNN**使用单变量函数作为滤波器，可以处理欧几里得结构和非欧几里得结构数据。

##### b. Gate
**GGNN**在传播步骤中使用GRU，进行固定步数的递归操作，并使用反向传播计算梯度。

**Child-Sum Tree-LSTM**和**N-ary Tree-LSTM**每个Tree-LSTM单元包含输入门、输出门、记忆单元和隐藏状态，并且为每个子类设置一个遗忘门，使得每个Tree-LSTM单元可以选择性地合并每个子类的信息。如果树最多有K个分支且每个节点的所有子节点都被排好序，则可以使用N-ary Tree-LSTM。与Child-Sum Tree-LSTM相比，N-ary Tree-LSTM为每个子类引入单独的参数矩阵，可以学习更多关于单元子类状态的细粒度表示。

N. Peng等人基于关系提取任务提出Graph LSTM的变体，最大的不同是图中的边有标签，并且使用不同的权重矩阵表示不同的标签。

**S-LSTM**目的是改进文本编码，将文本转换成图，使用Graph LSTM学习表示。

X. Liang等人提出的Graph LSTM模型旨在解决语义对象解析任务。它使用置信度驱动方案自适应地选择起始节点并确定节点更新顺序，有特定的更新顺序。

##### c. Attention
**GAT**在传播步骤中使用注意力机制。它通过关注它的邻居来计算每个节点的隐藏状态，遵循自注意力(self-attention)策略。GAT定义了图注意力层(graph attentional layer)，通过叠加该注意力层可以构建任意的图注意力网络。此外，该注意力层使用了multi-head注意力机制来稳定学习过程。它使用了K个独立的注意力机制来计算隐藏状态，然后将其特征连接起来(或计算平均值)。

**GAAN**也使用了multi-head注意力机制，但是它使用自注意力机制聚集来自不同head的信息，以代替GAT的平均计算。

##### d. Skip connection
**Highway GCN**使用类似于高速公路网络的分层闸，每一层的输出与其输入的选通权重相加。

**Column Network(CLN)**也利用了公路网，但是它使用不同的函数来计算门控权重。

**Jump Knowledge Network**可以学习自适应、结构感知的表示。模型从最后一层的每个节点的所有中间表示中进行选择，这使模型可以根据需要调整每个节点的有效邻域大小。此外，模型使用了concatenation, max-poolin和LSTM-attention三种方法汇总信息。

##### e. Hierarchical Pooling
**Edge-Conditioned Convolution(ECC)**设计了一个具有递归下采样操作的池化模块，下采样方法是根据拉普拉斯矩阵的最大特征向量的符号将图分解成两个分量。

DIFFPOOL通过在每一层训练一个分配矩阵，提出了一种可学习的层次聚类模型。

#### (3)Training Methods
##### a. Sampling
**GraphSAGE**使用可学习的聚合函数替换了完整的图形拉普拉斯算子，这是进行消息传递和推广到看不见的节点的关键。模型首先聚合邻域嵌入，然后与目标节点的嵌入合并，传播到下一层。通过学习到的聚合和传播功能，GraphSAGE可以为看不见的节点生成嵌入。此外，GraphSAGE使用邻域采样来缓解感受野的扩展。

PinSage提出了基于重要性的采样方法。通过模拟从目标节点开始的随机游走，选择标准化访问次数最高的前T个节点。

**FastGCN**直接对每一层的感受野进行采样，而不是采样每个节点的邻居，且根据重要性进行采样。

W. Huang等人介绍了一种参数化、可训练的采样器，可以在前一层的基础上进行分层采样。此外，该自适应采样器可以同时找到最优采样重要性和减少方差。

SSE提出了用于GNN训练的随机不动点梯度下降法。此方法将嵌入更新视为值函数，将参数更新视为值函数。在训练期间，该算法将对节点进行采样以更新嵌入，并对标记的节点进行采样以交替更新参数。

##### b. Receptive Field Control
J. Chen等人利用节点的历史激活作为控制变量，提出了一种基于控制变量的GCN随机逼近算法。
##### c. Data Augmentation
Co-Training GCN和Self-Training GCN用于扩大训练数据集。前者查找训练数据的最近邻，后者遵循类似Boosting的方法。
##### d. Unsupervised Training
图形自动编码器的目的是通过无监督的训练方式将节点表示为低维向量。**GAE(图形自动编码器)**首先使用GCN对图中的节点进行编码。然后使用一个简单的解码器来重建邻接矩阵，并根据原始邻接矩阵与重构矩阵之间的相似度来计算损失。

**Adversarially Regularized Graph Auto-encoder(ARGA)**运用生成对抗网络(GAN)来规范化基于GCN的图形自动编码器以遵循先前的分布。

### 3. General Frameworks
#### (1)Message Passing Neural Networks(MPNNs)
MPNNs是图形监督学习的一般性框架，抽象了几种最流行的图结构数据模型之间的共性。模型包含两个阶段，信息传递阶段和信息读出阶段。信息传递阶段(即传播步骤)运行T个时间步长，并根据信息函数和顶点更新函数进行定义。信息读出阶段使用读出函数计算整个图的特征向量。信息传递函数，顶点更新函数和读出函数可以有不同的设置，因此，MPNN框架可以通过不同的函数设置来概括几种不同的模型。

#### (2)Non-local Neural Networks(NLNN)
NLNN使用深度神经网络获取远程依赖关系。非局部运算是计算机视觉中经典的非局部均值运算的概括，非局部操作以所有位置特征的加权和计算一个位置的响应，这组位置可以是在空间、时间或时空中。因此NLNN可以看作是各种“自注意力”式方法的统一。

#### (3)Graph Networks(GN)
GN概括并扩展了各种图神经网络，MPNN和NLNN方法。
##### a. Graph definition
将图形定义为一个3元组，其中包括全局属性、节点集(包括节点属性)和边集(包括边属性、接收节点的索引和发送节点的索引)。
##### b. GN block
一个GN模块包含三个更新函数和三个聚合函数。
##### c. Computation steps
一个GN模块的计算步骤如下图所示：
![img](https://github.com/sunshinemingo/sunshinemingo.github.io/raw/master/img/image_md/image_38.png)
##### d. Design Principles
GN的设计基于三个基本原则：灵活的表示(GN框架支持属性和不同图结构的灵活表示)，可配置的块内结构(GN块中的函数及其输入可以有不同的设置，以便GN框架在块内结构配置中提供灵活性)和可组合的多块体系结构。
##### e. Composable multi-block architectures
GN块可以组成复杂的结构。任意数量的GN块可以按顺序组成，可以共享参数也可以不共享参数。

## 三、Applications
![img](https://github.com/sunshinemingo/sunshinemingo.github.io/raw/master/img/image_md/image_39.png)
### 1. Structural Scenarios
结构化场景：数据具有明确的关系结构
#### (1)Physics
对现实世界的物理系统进行建模是了解人工智能的最基本方面之一。通过将对象表示为节点，将关系表示为边，可以以一种简单而有效的方式对对象、关系和物理进行基于GNN的推理。

**Interaction Networks**对各种物理系统进行预测和推断。该模型以对象和关系为输入，对它们之间的相互作用进行推理，并应用效果和物理动力学来预测新的状态。它们分别对以关系为中心的模型和以对象为中心的模型进行建模，这使得跨不同系统的泛化更加容易。

**Visual Interaction Networks**可以通过像素进行预测。它从每个对象的两个连续输入帧中学习状态码。然后，通过一个交互网块增加它们的交互效果，之后状态解码器将状态码转换为下一步的状态。

#### (2)Chemistry and Biology
##### a. Molecular Fingerprints
分子指纹计算，是指分子特征向量的计算，它是计算机辅助药物设计的核心步骤。传统的分子指纹是手工制作并且是固定的。将GNN应用于分子图，可以获得较好的指纹图谱。

**neural graph fingerprints**通过GCN计算子结构的特征向量并求和，得到整体表示。

##### b. Protein Interface Prediction
**Protein Interface Prediction**是一个具有挑战性的问题，在药物发现和设计中有着重要的应用。提出的基于GCN的方法分别学习配体和受体蛋白残基的表示，并将它们融合在一起进行两两分类。

#### (3)Knowledge graph
GNNs被用于解决知识库补全(KBC)中的知识库外实体(OOKB)问题。GCNs可以用于解决跨语言知识图谱对齐问题。该模型将不同语言的实体嵌入到一个统一的嵌入空间中，并根据嵌入相似度对它们进行对齐。

### 2. Non-structural Scenarios
非结构化场景的关系结构不明确。大致有两种方法可以将图神经网络应用于非结构化场景：

**·** 结合其他领域的结构信息来提高性能；

**·** 在场景中推断或假设关系结构，然后使用该模型解决图上定义的问题。

#### (1)Image
##### a. Image Classification
图像分类的最新进展得益于大数据和GPU计算的强大功能，可以在不从图像中提取结构信息的情况下训练分类器。然而zero-shot和few-shot学习在图像分类领域中越来越受欢迎，因为大部分模型都可以在足够的数据下获得相似的性能。

**·** 知识图谱可以作为额外信息指导zero-shot识别分类。

**·** 数据集中图像之间的相似性也有助于进行few-shot学习。

##### b. Visual Reasoning
计算机视觉系统通常需要结合空间和语义信息来进行推理。一个典型的视觉推理任务是视觉问题回答(VQA)，其他应用包括目标检测，交互检测和区域分类。

##### c. Semantic Segmentation
语义分割是进行图像理解的关键步骤。它的任务是为图像中的每个像素分配一个唯一的标签(或类别)，可以视为密集分类问题。但是，图像中的区域通常不是网格状的，并且需要非局部信息，这导致了传统CNN的失败。

#### (2)Text
图神经网络可以应用于句子级任务(例如文本分类)以及词级任务(例如序列标签)。
##### a. Text classification
文本分类是自然语言处理中一个重要而经典的问题。经典的GCN模型和GAT模型被用来来解决这个问题，但是它们只使用了文档之间的结构信息，而没有使用太多的文本信息。
##### b. Semantic Segmentation 
由于GNN中的每个节点都有其隐藏状态，如果将句子中的每个单词都视为一个节点，则可以利用隐藏状态来解决序列标注问题。语义角色标注是序列标注的另一项任务。
##### c. Neural machine translation
神经机器翻译任务被认为是一个序列到序列(sequence-to-sequence)的任务。
##### d. Relation extraction
提取文本中实体之间的语义关系是一项重要且需要进行充分研究的任务。一些系统将这个任务看成两个独立的任务，分别称为实体识别和关系提取。
##### e. Event extraction
事件提取是一项重要的信息提取任务，可以用于识别文本中指定类型事件的实例。

### 3. Other Scenarios
#### (1)Generative Models
现实世界中图形的生成模型因其重要的应用而引起了极大的关注，包括社交交互建模，新化学结构发现以及知识图谱构建。而且深度学习方法具有学习图形隐式分布的强大功能，因此有很多图神经网络生成模型。

#### (2)Combinatorial Optimization
图上的组合优化问题是一类NP难题，引起了各个领域科学家的广泛关注。一些特定的问题，例如旅行商问题(TSP)和最小生成树(MST)，已经有了各种启发式解决方案。近年来，使用深度神经网络解决此类问题已成为热点，并且一些解决方案由于其图结构而进一步利用了图神经网络。

## 四、Open problems
### 1. Shallow Structure
传统的深度神经网络可以堆叠数百层以获得更好的性能，因为更深的结构有更多的参数，从而显著提高了表达能力。但是，图神经网络往往是浅层的，大多不超过三层。有实验表明，堆叠多个GCN层将导致过度平滑，也就是说，所有顶点都将收敛到相同的值。尽管有一些研究人员设法解决了这个问题，但它仍然是GNN最大的局限。
### 2. Dynamic Graphs
处理具有动态结构的图是一个具有挑战性的问题。静态图是稳定的，可以对其进行建模，而动态图则引入了变化的结构。当边和节点出现或消失时，GNN无法自适应变化。
### 3. Non-Structural Scenarios
文中对GNN在非结构化场景中的应用进行了阐述，但是没有发现从原始数据生成图形的最佳方法。在图像领域，有的利用CNN获取特征图，然后对其进行上采样，形成超像素作为节点，而有的直接利用一些目标检测算法获取对象节点。在文本域中，有的采用句法树作为句法图，有的采用全连接图。
### 4. Scalability
如何在社交网络或推荐系统等网络规模条件下应用嵌入方法，是几乎所有图形嵌入算法都面临的一个致命问题，GNN也不例外。扩展GNN十分困难，因为许多核心步骤在大数据环境中消耗大量计算资源。

## 五、Conclusion
在过去的几年中，图神经网络已经成为强大和实用的工具，可以用于图形领域的机器学习任务。这一进步归因于表达能力，模型灵活性和训练算法的提高。 文中对图神经网络进行了全面的回顾，介绍了GNN模型按图形类型，传播类型和训练类型分类的变体；总结了几个通用框架以统一表示不同的变体。在应用分类方面，GNN的应用可以分为结构性场景，非结构性场景和其他场景，然后对每种场景中的应用进行了详细的回顾。最后，提出了四个开放性问题，这些问题指出了图神经网络的主要挑战和未来研究方向，包括模型深度，可伸缩性，处理动态图的能力和非结构化场景。
